<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="google9aedf817252bb42b"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Kwan-Yee Lin</title> <meta name="author" content="Kwan-Yee Lin"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/milky-way.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kwanyeelin.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6C%69%6E%6A%75%6E%79%69%39%33%33%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=beGt3cAAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/ky_lin0305" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publication/">Publication</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Industry</a> </li> <li class="nav-item "> <a class="nav-link" href="/dataset/">Dataset</a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">Software</a> </li> <li class="nav-item "> <a class="nav-link" href="/Others/">Others</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Kwan-Yee Lin </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/kwanyee_lin-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/kwanyee_lin-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/kwanyee_lin-1400.webp"></source> <img class="img-fluid z-dept-100 rounded" src="/assets/img/kwanyee_lin.jpg" alt="kwanyee_lin.jpg"> </picture> </figure> </div> <div class="clearfix"> <p>I am a Research Fellow in the EECS department at the University of Michigan, Ann Arbor, working with <a href="https://web.eecs.umich.edu/~stellayu/" target="_blank" rel="noopener noreferrer">Stella X. Yu</a>. Previously, I was a Postdoctoral Researcher in MMLab at The Chinese University of Hong Kong, working with <a href="https://www.ee.cuhk.edu.hk/~xgwang/" target="_blank" rel="noopener noreferrer">Xiaogang Wang</a> and <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank" rel="noopener noreferrer">Hongsheng Li</a>. I was a Director of R&amp;D at Intelligent Automotive Group of <a href="https://www.sensetime.com/en/" target="_blank" rel="noopener noreferrer">SenseTime Group Inc.</a>, working with <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Chen Qian</a>. I received my PhD in the Department of Information Science, School of Mathematical Sciences at Peking University.</p> <div class="research"> <h2>Research</h2> <p>My research interests lie at the intersection of Computer Vision, Computer Graphics, Robotics, and Machine Learning. My long-term goal is to build <strong>Realistic and Intelligent Virtual Humans</strong> -- embodied human-like systems that not only resemble humans, but also emulate humans' remarkable cognitive and motor capabilities, enabling them to interact with the world with self-autonomy. I view and approach "Virtual Humans" from two complementary perspectives: (1) Virtual Avatars, which focus on digital representations of humans. (2) Humanoid Robots, which serve as the embodied carriers to connect perception to action in the physical world. My research has thus three themes:</p> <ul> <li> <strong>Realistic Virtual Humans</strong>: Constructing high-fidelity 4D volumetric capture systems and datasets, as in <a href="https://dna-rendering.github.io/" target="_blank" rel="noopener noreferrer">DNA-Rendering</a> and <a href="https://renderme-360.github.io/" target="_blank" rel="noopener noreferrer">RenderMe-360</a>; modeling realistic human appearances and motions, as in <a href="https://yzmblog.github.io/projects/MonoHuman/" target="_blank" rel="noopener noreferrer">MonoHuman</a>, <a href="https://arxiv.org/abs/2105.02431" target="_blank" rel="noopener noreferrer">GAR</a>, and <a href="https://timewalker2024.github.io/timewalker.github.io/" target="_blank" rel="noopener noreferrer">TimeWalker</a>; learning context-aware behaviour generation, as in <a href="publication/">EmbodiedHuman</a>; and developing human foundation models to obtain generic representations, as in <a href="https://cosmicman-cvpr2024.github.io/" target="_blank" rel="noopener noreferrer">CosmicMan</a>.</li> <li> <strong>All-round Humanoid Robots</strong>: Building the synergistic relations of visual perception, proprioception, and action in embodied agents to develop integrative skills autonomously, as in <a href="https://lego-h-humanoidrobothiking.github.io/" target="_blank" rel="noopener noreferrer">LEGO-H</a>.</li> <li> <strong>Steerable Scene Constrution</strong>: Developing steerable scene modeling and simulation frameworks, as a playground, to potentially enhance perception, navigation, and interaction in complex environments, as in <a href="https://dnmp.github.io/" target="_blank" rel="noopener noreferrer">UrbanRF (DNMP)</a>, <a href="https://urbanarchitect.github.io/" target="_blank" rel="noopener noreferrer">UrbanArchitect</a>, and <a href="https://slr-sfs.github.io/" target="_blank" rel="noopener noreferrer">SLR-SFS</a>.</li> </ul> </div> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <td class="date">Mar, 2025</td> <td class="announcement"> We are organizing the workshop on <a href="https://poets2024.github.io/poets2025/" target="_blank" rel="noopener noreferrer">Embodied ‚ÄúHumans‚Äù: Symbiotic Intelligence Between Virtual Humans and Humanoid Robots</a> at CVPR 2025. üî• </td> </tr> <tr> <td class="date">Dec, 2024</td> <td class="announcement"> <strong>2</strong> papers with <strong>1</strong> highlight presentation got accepted by CVPR/ECCV 2024. </td> </tr> <tr> <td class="date">Mar, 2024</td> <td class="announcement"> We are now organizing the first POETS workshop on <a href="https://poets2024.github.io/" target="_blank" rel="noopener noreferrer">Virtual Humans for Robotics and Autonomous Driving @ CVPR 2024</a>! </td> </tr> <tr> <td class="date">Sep, 2023</td> <td class="announcement"> The part-2 of <a href="https://dna-rendering.github.io/" target="_blank" rel="noopener noreferrer">DNA-Rendering</a> dataset is released. </td> </tr> <tr> <td class="date">Sep, 2023</td> <td class="announcement"> <a href="https://renderme-360.github.io/" target="_blank" rel="noopener noreferrer">RenderMe-360</a>, a large-scale dataset for high-fidelity head avatar rendering, is accepted by NeuIPS 2023! üî• </td> </tr> <tr> <td class="date">Jul, 2023</td> <td class="announcement"> <strong>6</strong> papers with <strong>1</strong> oral presentations got accepted by CVPR/ICCV 2023. </td> </tr> <tr> <td class="date">Sep, 2022</td> <td class="announcement"> We are starting <a href="https://openxdlab.org.cn/home" target="_blank" rel="noopener noreferrer">OpenXDLab</a>, a new large-scale open-source project for datasets of XR research! üî• </td> </tr> <tr> <td class="date">Jul, 2022</td> <td class="announcement"> <strong>3</strong> papers with <strong>1</strong> oral presentations got accepted by CVPR/ECCV 2022. </td> </tr> <tr> <td class="date">Mar, 2021</td> <td class="announcement"> <strong>2</strong> papers with <strong>1</strong> oral presentations got accepted by CVPR 2021. </td> </tr> <tr> <td class="date">Jul, 2020</td> <td class="announcement"> <strong>2</strong> papers got accepted by CVPR/ECCV in 2020. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/kwanyee2025legoh.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/kwanyee2025legoh.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/kwanyee2025legoh.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="kwanyee2025legoh" class="col-sm-8"> <div class="title">Let Humanoid Robots Go Hiking! Integrative Skill Development over Complex Trails</div> <div class="author"> <em>Kwan-Yee Lin</em>,¬†and Stella X. Yu </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2025 </div> <div class="links"> <a href="https://lego-h-humanoidrobothiking.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/shikai2024cosmicman.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/shikai2024cosmicman.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/shikai2024cosmicman.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="shikai2024cosmicman" class="col-sm-8"> <div class="title">CosmicMan: A Text-to-Image Foundation Model for Humans</div> <div class="author">Shikai Li,¬†Jianglin Fu,¬†Kaiyuan Liu,¬†Wentao Wang,¬† <em>Kwan-Yee Lin</em> ‚Ä†,¬†and Wayne Wu </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition (CVPR),</em> 2024 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=fk2fniU6oyM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://cosmicman-cvpr2024.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <div class="highlight-label" style="margin-top: 8px; text-align: left;"> <strong style="color: red; font-size: 1.1em;">Highlight</strong> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/contributors2023renderme360.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/contributors2023renderme360.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/contributors2023renderme360.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="contributors2023renderme360" class="col-sm-8"> <div class="title">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</div> <div class="author">Dongwei Pan,¬† etal.,¬†and <em>Kwan-Yee Lin</em> </div> <div class="periodical"> <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2305.13353" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIgrtQwkrdg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://renderme-360.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/contributors2023dnarendering.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/contributors2023dnarendering.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/contributors2023dnarendering.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="contributors2023dnarendering" class="col-sm-8"> <div class="title">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</div> <div class="author">Wei Cheng,¬† etal.,¬†and <em>Kwan-Yee Lin</em> </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2307.10173" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=xlhfvxvu7nc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://dna-rendering.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/fan2023urban.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/fan2023urban.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/fan2023urban.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="fan2023urban" class="col-sm-8"> <div class="title">Urban Radiance Field Representation with Deformable Neural Mesh Primitives</div> <div class="author">Fan Lu,¬†Yan Xu,¬†Guang Chen,¬†Hongsheng Li,¬† <em>Kwan-Yee Lin</em> ‚Ä†,¬†and Changjun Jiang </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2307.10776" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=JABhlaVq4VA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://dnmp.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/slrsmf2022.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/slrsmf2022.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/slrsmf2022.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="slrsmf2022" class="col-sm-8"> <div class="title">Simulating Fluids in Real-World Still Images</div> <div class="author">Siming Fan,¬†Jingtan Piao,¬†Chen Qian,¬†Hongsheng Li,¬†and <em>Kwan-Yee Lin</em> </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11335" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=Aatrl16t-V8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://slr-sfs.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> <div class="oral-label" style="margin-top: 8px; text-align: left;"> <strong style="color: red; font-size: 1.1em;">Oral</strong> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/wei2022gnr.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/wei2022gnr.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/wei2022gnr.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="wei2022gnr" class="col-sm-8"> <div class="title">Generalizable Neural Performer: Learning Robust Radiance Fields for Human Novel View Synthesis</div> <div class="author">Wei Cheng,¬†Su Xu,¬†Jingtan Piao,¬†Chen Qian,¬†Wayne Wu,¬† <em>Kwan-Yee Lin</em> ‚Ä†,¬†and Hongsheng Li </div> <div class="periodical"> <em>Technical report, arXiv:2204.11798,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11798" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=2COR4u1ZIuk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://generalizable-neural-performer.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/jianglin2022styleganhuman.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/jianglin2022styleganhuman.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/jianglin2022styleganhuman.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="jianglin2022styleganhuman" class="col-sm-8"> <div class="title">StyleGAN-Human: A Data-Centric Odyssey of Human Generation</div> <div class="author">Jianglin Fu,¬†Shikai Li,¬†Yuming Jiang,¬† <em>Kwan-Yee Lin</em>,¬†Chen Qian,¬†Chen Change Loy,¬†Wayne Wu,¬†and Ziwei Liu </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIrb9hwsdcI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://stylegan-human.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/sscyjc2021.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/sscyjc2021.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/sscyjc2021.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="sscyjc2021" class="col-sm-8"> <div class="title">Semantic Scene Completion via Integrating Instances and Scene in-the-Loop</div> <div class="author">Yingjie Cai,¬†Xuesong Chen,¬†Chao Zhang,¬† <em>Kwan-Yee Lin</em> ‚Ä†,¬†Xiaogang Wang,¬†and Hongsheng Li </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2104.03640" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/yjcaimeow/SISNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/sketchsscxk2020-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/sketchsscxk2020-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/sketchsscxk2020-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/sketchsscxk2020.png"> </picture> </figure> </div> <div id="sketchsscxk2020" class="col-sm-8"> <div class="title">3D Sketch-aware Semantic Scene Completion via Semi-supervised Structure Prior</div> <div class="author">Xiaokang Chen,¬† <em>Kwan-Yee Lin</em>,¬†Chen Qian,¬†Gang Zeng,¬†and Hongsheng Li </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2003.14052" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/charlesCXK/TorchSSC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <div class="video-container"> <video class="img-fluid z-dept-1 rounded" autoplay loop muted poster="/assets/img/publication/3dhumanposekylin2019.png" preload="metadata" playsinline webkit-playsinline onmouseover="this.controls=true" onmouseout="this.controls=false"><source src="/assets/videos/3dhumanposekylin2019.mp4" type="video/mp4"><p>Your browser does not support the video tag. <a href="/assets/videos/3dhumanposekylin2019.mp4">Download the video</a> instead.</p> </source></video> </div> </figure> <style>.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;background-color:#f8f9fa;border-radius:8px;overflow:hidden}.video-container video{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:8px;background-color:#000}.video-container video::-webkit-media-controls{background-color:rgba(0,0,0,0.5)}.video-container video::-webkit-media-controls-panel{background-color:transparent}.video-container video::-webkit-media-controls-play-button{background-color:transparent}.video-container video::-webkit-media-controls-timeline{background-color:transparent}.video-container video{background-color:#000!important;background-image:none!important;object-fit:cover;-webkit-appearance:none;appearance:none}.video-container video::-webkit-media-controls{background-color:transparent!important}.video-container video::-webkit-media-controls-panel{background-color:transparent!important}.video-container video::-webkit-media-controls-overlay-play-button{background-color:transparent!important}.video-container video::-webkit-media-controls-start-playback-button{background-color:transparent!important}.video-container video::-webkit-media-controls-enclosure{background-color:transparent!important}.video-container video::-webkit-media-controls-fullscreen-button{background-color:transparent!important}.video-container video::-webkit-media-controls-timeline{background-color:transparent!important}.video-container video::-webkit-media-controls-volume-slider{background-color:transparent!important}.video-container video::-webkit-media-controls-mute-button{background-color:transparent!important}.video-container video[poster]{background-color:#000!important;background-image:none!important}.video-container video::-webkit-media-controls-container{background-color:transparent!important}</style> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".video-container video").forEach(function(e){e.style.backgroundColor="#000",e.style.backgroundImage="none",e.style.webkitAppearance="none",e.style.appearance="none",e.poster&&(e.style.backgroundImage="url("+e.poster+")",e.style.backgroundSize="cover",e.style.backgroundPosition="center");["loadstart","loadedmetadata","loadeddata","canplay","canplaythrough","progress"].forEach(function(t){e.addEventListener(t,function(){this.style.backgroundColor="#000",this.style.backgroundImage=this.poster?"url("+this.poster+")":"none"})}),e.addEventListener("click",function(){this.style.backgroundColor="#000"}),e.addEventListener("mouseenter",function(){this.style.backgroundColor="#000"});new MutationObserver(function(t){t.forEach(function(t){"attributes"===t.type&&"style"===t.attributeName&&(e.style.backgroundColor="#000",e.style.backgroundImage=e.poster?"url("+e.poster+")":"none")})}).observe(e,{attributes:!0,attributeFilter:["style"]})})});</script> </div> <div id="3dhumanposekylin2019" class="col-sm-8"> <div class="title">Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation</div> <div class="author"> <em>Kwan-Yee Lin</em>,¬†Xipeng Chen,¬†Wentao Liu,¬†Chen Qian,¬†and Liang Lin </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1903.08839" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://kwanyeelin.github.io/projects/GeoRep-3DPose/GeoRep-3DPose.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> <div class="oral-label" style="margin-top: 8px; text-align: left;"> <strong style="color: red; font-size: 1.1em;">Oral</strong> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Kwan-Yee Lin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: September 08, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4QS9LCB68Y"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4QS9LCB68Y");</script> </body> </html>